{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Read data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dziękuję za dokumentację wstępną. Cieszę się żę uwzględnili Państwo moje uwagi odnośnie dodatkowych algorytmów optymalizacji.\n",
    "Z drobnych uwag to polecałbym skorzystać z gotowej biblioteki do algorytmów genetycznych np. PyGAD\n",
    "W dokumentacji brakuje mi informacji na temat podziału zbioru danych na treningowy (na którym otpymalizować powinni Państwo knn-a, walidacyjnym na którym obliczane będą metryki i testowy - który wykorzystany będzie do wyznaczenia końcowej jakości rozwiązania)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "def get_data():\n",
    "    # Assuming 'farm-ads-vect' is in SVMlight format\n",
    "    X, y = load_svmlight_file('data/farm-ads-vect')\n",
    "\n",
    "    # Convert sparse matrix to DataFrame\n",
    "    df_vectors = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "\n",
    "    return df_vectors, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df, labels = get_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   0      1      2      3      4      5      6      7      8      9      ...  \\\n0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    1.0    0.0  ...   \n1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  ...   \n2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0  ...   \n\n   54867  54868  54869  54870  54871  54872  54873  54874  54875  54876  \n0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n\n[5 rows x 54877 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>54867</th>\n      <th>54868</th>\n      <th>54869</th>\n      <th>54870</th>\n      <th>54871</th>\n      <th>54872</th>\n      <th>54873</th>\n      <th>54874</th>\n      <th>54875</th>\n      <th>54876</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 54877 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(4143, 54877)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a logistic regression model on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained logistic regression model\n",
    "    - X_test: DataFrame, feature vectors for testing\n",
    "    - y_test: Series, true labels for testing\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: float, accuracy of the model on the test set\n",
    "    - report: str, classification report (includes precision, recall, f1-score, and support)\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, f1\n",
    "\n",
    "\n",
    "def train_knn(X_train, y_train, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Train a k-nearest neighbors (KNN) model on the training data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame, feature vectors for training\n",
    "    - y_train: Series, labels for training\n",
    "    - n_neighbors: int, number of neighbors to consider (default is 5)\n",
    "\n",
    "    Returns:\n",
    "    - model: trained KNN model\n",
    "    \"\"\"\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def evaluate_attribute_set(attribute_names: List[str], df, labels):\n",
    "    df_temp = df[attribute_names]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_temp, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the logistic regression model\n",
    "    model = train_knn(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, f1 = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    return accuracy, f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def sequential_forward_selection(df, labels, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform Sequential Forward Selection (SFS) for feature selection.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame, feature vectors for training\n",
    "    - y_train: Series, labels for training\n",
    "    - X_test: DataFrame, feature vectors for testing\n",
    "    - y_test: Series, true labels for testing\n",
    "    - max_features: int, maximum number of features to select (default is None)\n",
    "\n",
    "    Returns:\n",
    "    - best_features: list, selected features\n",
    "    \"\"\"\n",
    "    num_features = df.shape[1]\n",
    "    all_features = list(df.columns)\n",
    "    selected_features = []\n",
    "    best_accuracy = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    df_performance = pd.DataFrame(columns=[\"selected_features\", \"num_selected_features\", \"time\", \"accuracy_validate\"])\n",
    "    start_time = time()\n",
    "    while len(selected_features) < num_features and (max_features is None or len(selected_features) < max_features):\n",
    "        remaining_features = [feature for feature in all_features if feature not in selected_features]\n",
    "        current_best_feature = None\n",
    "\n",
    "        for feature in remaining_features:\n",
    "            trial_features = selected_features + [feature]\n",
    "            accuracy, f1 = evaluate_attribute_set(trial_features, df, labels)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_f1 = f1\n",
    "                current_best_feature = feature\n",
    "\n",
    "        if current_best_feature is not None:\n",
    "            selected_features.append(current_best_feature)\n",
    "            print(f\"Selected Features: {selected_features}\")\n",
    "            print(f\"Accuracy with Selected Features: {best_accuracy:.2f}\")\n",
    "            df_performance = pd.concat([df_performance, pd.DataFrame({\n",
    "                \"selected_features\": selected_features.copy(),\n",
    "                \"num_selected_features\": len(selected_features),\n",
    "                \"time\": time() - start_time,\n",
    "                \"accuracy_validate\": best_accuracy,\n",
    "                \"f1_validate\": best_f1\n",
    "            })])\n",
    "\n",
    "    return selected_features, best_accuracy, best_f1, df_performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "N_OF_FEATURES_TO_TEST = 100\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:N_OF_FEATURES_TO_TEST], labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [15]\n",
      "Accuracy with Selected Features: 0.64\n",
      "Selected Features: [15, 78]\n",
      "Accuracy with Selected Features: 0.71\n",
      "Selected Features: [15, 78, 54]\n",
      "Accuracy with Selected Features: 0.72\n",
      "Selected Features: [15, 78, 54, 66]\n",
      "Accuracy with Selected Features: 0.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[103], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m selected_features, best_accuracy, best_f1, df_performance \u001B[38;5;241m=\u001B[39m \u001B[43msequential_forward_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_ITER\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m selected_features, best_accuracy\n",
      "Cell \u001B[1;32mIn[98], line 29\u001B[0m, in \u001B[0;36msequential_forward_selection\u001B[1;34m(df, labels, max_iter)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m remaining_features:\n\u001B[0;32m     28\u001B[0m     trial_features \u001B[38;5;241m=\u001B[39m selected_features \u001B[38;5;241m+\u001B[39m [feature]\n\u001B[1;32m---> 29\u001B[0m     accuracy, f1 \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_attribute_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accuracy \u001B[38;5;241m>\u001B[39m best_accuracy:\n\u001B[0;32m     32\u001B[0m         best_accuracy \u001B[38;5;241m=\u001B[39m accuracy\n",
      "Cell \u001B[1;32mIn[85], line 13\u001B[0m, in \u001B[0;36mevaluate_attribute_set\u001B[1;34m(attribute_names, df, labels)\u001B[0m\n\u001B[0;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m train_knn(X_train, y_train)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m accuracy, f1 \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy, f1\n",
      "Cell \u001B[1;32mIn[84], line 18\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(model, X_test, y_test)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_model\u001B[39m(model, X_test, y_test):\n\u001B[0;32m      6\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m    Evaluate a logistic regression model on the test data.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m    - report: str, classification report (includes precision, recall, f1-score, and support)\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n\u001B[0;32m     20\u001B[0m     f1 \u001B[38;5;241m=\u001B[39m f1_score(y_test, y_pred)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001B[0m, in \u001B[0;36mKNeighborsClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m    Class labels for each data sample.\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;66;03m# In that case, we do not need the distances to perform\u001B[39;00m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;66;03m# the weighting so we do not compute them.\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m     neigh_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m     neigh_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_base.py:824\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    817\u001B[0m use_pairwise_distances_reductions \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    818\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrute\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    819\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m ArgKmin\u001B[38;5;241m.\u001B[39mis_usable_for(\n\u001B[0;32m    820\u001B[0m         X \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meffective_metric_\n\u001B[0;32m    821\u001B[0m     )\n\u001B[0;32m    822\u001B[0m )\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_pairwise_distances_reductions:\n\u001B[1;32m--> 824\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mArgKmin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    828\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meffective_metric_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    829\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetric_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meffective_metric_params_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_distance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    834\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[0;32m    835\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrute\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetric \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecomputed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m issparse(X)\n\u001B[0;32m    836\u001B[0m ):\n\u001B[0;32m    837\u001B[0m     results \u001B[38;5;241m=\u001B[39m _kneighbors_from_graph(\n\u001B[0;32m    838\u001B[0m         X, n_neighbors\u001B[38;5;241m=\u001B[39mn_neighbors, return_distance\u001B[38;5;241m=\u001B[39mreturn_distance\n\u001B[0;32m    839\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:277\u001B[0m, in \u001B[0;36mArgKmin.compute\u001B[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \n\u001B[0;32m    198\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;124;03mreturns.\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m Y\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat64:\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mArgKmin64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetric_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_distance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m Y\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat32:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ArgKmin32\u001B[38;5;241m.\u001B[39mcompute(\n\u001B[0;32m    290\u001B[0m         X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    291\u001B[0m         Y\u001B[38;5;241m=\u001B[39mY,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    297\u001B[0m         return_distance\u001B[38;5;241m=\u001B[39mreturn_distance,\n\u001B[0;32m    298\u001B[0m     )\n",
      "File \u001B[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:95\u001B[0m, in \u001B[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\threadpoolctl.py:171\u001B[0m, in \u001B[0;36m_ThreadpoolLimiter.__exit__\u001B[1;34m(self, type, value, traceback)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mtype\u001B[39m, value, traceback):\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestore_original_limits()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrap\u001B[39m(\u001B[38;5;28mcls\u001B[39m, controller, \u001B[38;5;241m*\u001B[39m, limits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, user_api\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "selected_features, best_accuracy, best_f1, df_performance = sequential_forward_selection(X_train, y_train, max_features=5)\n",
    "selected_features, best_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulated annealing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "def simulated_annealing(df, labels, initial_temp, final_temp, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Perform Simulated Annealing for feature selection.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        The dataset containing all potential features.\n",
    "    labels : Series or array-like\n",
    "        The target variable for the dataset.\n",
    "    initial_temp : float\n",
    "        The initial temperature for the annealing process.\n",
    "    final_temp : float\n",
    "        The final temperature at which the annealing process will stop.\n",
    "    alpha : float\n",
    "        The cooling rate of the temperature after each iteration.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "    best_set : list\n",
    "        The best set of features found.\n",
    "    best_accuracy : float\n",
    "        The best accuracy achieved with the best set of features.\n",
    "    best_f1 : float\n",
    "        The best F1 score achieved with the best set of features.\n",
    "    df_benchmark : DataFrame\n",
    "        A DataFrame recording the benchmark data of each iteration, including feature count, feature set, accuracy, F1 score, and other relevant metrics.\n",
    "    \"\"\"\n",
    "    BENCHMARK_COLS = [\"Iteration\", \"Feature Count\", \"Feature Set\", \"Accuracy\", \"F1\", \"Acceptance Probability\", \"Random Number\", \"Operation\", \"Improved\"]\n",
    "    df_benchmark = pd.DataFrame(columns=BENCHMARK_COLS)\n",
    "\n",
    "    current_temp = initial_temp\n",
    "    num_features = df.shape[1]\n",
    "    all_features = list(df.columns)\n",
    "    current_set = random.sample(all_features, k=random.randint(1, num_features))  # Random initial set\n",
    "\n",
    "    best_set = current_set.copy()\n",
    "    best_accuracy, best_f1 = evaluate_attribute_set(best_set, df, labels)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        if current_temp <= final_temp:\n",
    "            break\n",
    "\n",
    "        new_set, operation = make_random_change(current_set, all_features)\n",
    "        accuracy, f1 = evaluate_attribute_set(new_set, df, labels)\n",
    "\n",
    "        decision, random_num, probability = accept_change(best_accuracy, accuracy, current_temp)\n",
    "        if decision:\n",
    "            current_set = new_set\n",
    "            if accuracy > best_accuracy:\n",
    "                best_set = current_set\n",
    "                best_accuracy = accuracy\n",
    "                best_f1 = f1\n",
    "\n",
    "        current_temp *= alpha  # Cooling step\n",
    "\n",
    "        # \"Iteration\", \"Feature Count\", \"Feature Set\", \"Accuracy\", \"F1\", \"Acceptance Probability\", \"Random Number\", \"Operation\", \"Improved\"\n",
    "        iteration_data = pd.DataFrame(columns=BENCHMARK_COLS,\n",
    "                                      data=[[iteration, len(current_set), current_set, accuracy, f1, probability, random_num, operation, int(decision)]])\n",
    "\n",
    "        df_benchmark = pd.concat([df_benchmark, iteration_data])\n",
    "    return best_set, best_accuracy, best_f1, df_benchmark\n",
    "\n",
    "def make_random_change(current_set, all_features):\n",
    "    \"\"\"\n",
    "    Make a random change to the current set of features.\n",
    "\n",
    "    Parameters:\n",
    "    current_set : list\n",
    "        The current set of selected features.\n",
    "    all_features : list\n",
    "        The list of all possible features.\n",
    "\n",
    "    Returns:\n",
    "    current_set : list\n",
    "        The modified set of features after the random change.\n",
    "    operation : str\n",
    "        The type of operation performed ('add', 'remove', or 'swap').\n",
    "    \"\"\"\n",
    "    operation = random.choice([\"add\", \"remove\", \"swap\"])\n",
    "    if operation == \"add\":\n",
    "        # Add a random feature not in the current set\n",
    "        possible_additions = list(set(all_features) - set(current_set))\n",
    "        if possible_additions:\n",
    "            feature_to_add = random.choice(possible_additions)\n",
    "            current_set.append(feature_to_add)\n",
    "    elif operation == \"remove\":\n",
    "        # Remove a random feature from the current set\n",
    "        if current_set:\n",
    "            feature_to_remove = random.choice(current_set)\n",
    "            current_set.remove(feature_to_remove)\n",
    "    elif operation == \"swap\":\n",
    "        # Swap a feature in the set with one outside it\n",
    "        if current_set and len(all_features) > len(current_set):\n",
    "            feature_to_remove = random.choice(current_set)\n",
    "            current_set.remove(feature_to_remove)\n",
    "            possible_additions = list(set(all_features) - set(current_set))\n",
    "            feature_to_add = random.choice(possible_additions)\n",
    "            current_set.append(feature_to_add)\n",
    "    return current_set, operation\n",
    "\n",
    "def accept_change(best_accuracy, new_accuracy, temperature):\n",
    "    \"\"\"\n",
    "    Determine whether to accept the new set of features based on the Simulated Annealing acceptance criterion.\n",
    "\n",
    "    Parameters:\n",
    "    best_accuracy : float\n",
    "        The best accuracy achieved so far.\n",
    "    new_accuracy : float\n",
    "        The accuracy achieved with the new set of features.\n",
    "    temperature : float\n",
    "        The current temperature in the annealing process.\n",
    "\n",
    "    Returns:\n",
    "    decision : bool\n",
    "        A boolean indicating whether the new set of features is accepted.\n",
    "    random_num : float\n",
    "        A random number generated for the probabilistic decision.\n",
    "    probability : float\n",
    "        The calculated probability used to determine the acceptance of the new feature set.\n",
    "    \"\"\"\n",
    "    probability = 0\n",
    "    random_num = 0\n",
    "    if new_accuracy > best_accuracy:\n",
    "        decision = True # Always accept if the new solution is better\n",
    "    else:\n",
    "        # Calculate the probability of acceptance and accept based on it\n",
    "        probability = np.exp((new_accuracy - best_accuracy) / temperature)\n",
    "        random_num = random.random()\n",
    "        decision = random.random() < probability\n",
    "    return decision, random_num, probability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "N_OF_FEATURES_TO_TEST = 1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:N_OF_FEATURES_TO_TEST], labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8235294117647058, 0.8354430379746837)"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "MAX_ITER=50\n",
    "\n",
    "best_set, best_accuracy, best_f1, df_benchmark = simulated_annealing(X_train, y_train,\n",
    "                                                                     initial_temp=1,\n",
    "                                                                     final_temp=0.01,\n",
    "                                                                     alpha=0.95,\n",
    "                                                                     max_iter=MAX_ITER)\n",
    "best_accuracy, best_f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "  Iteration Feature Count                                        Feature Set  \\\n0         0           250  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         1           249  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         2           250  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         3           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         4           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         5           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         6           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         7           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         8           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0         9           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        10           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        11           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        12           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        13           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        14           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        15           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        16           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        17           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        18           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        19           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        20           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        21           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        22           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        23           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        24           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        25           250  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        26           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        27           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        28           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        29           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        30           250  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        31           250  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        32           251  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        33           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        34           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        35           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        36           252  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        37           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        38           254  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        39           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        40           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        41           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        42           254  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        43           253  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        44           254  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        45           255  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        46           255  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        47           256  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        48           257  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n0        49           258  [967, 51, 649, 840, 270, 776, 415, 155, 331, 8...   \n\n   Accuracy        F1  Acceptance Probability  Random Number Operation  \\\n0  0.817496  0.832178                0.995485       0.988850       add   \n0  0.819005  0.833333                0.996830       0.530471    remove   \n0  0.817496  0.832178                0.994999       0.379146       add   \n0  0.820513  0.832630                0.998242       0.425876       add   \n0  0.820513  0.832630                0.998150       0.999881       add   \n0  0.820513  0.832630                0.998053       0.856608    remove   \n0  0.823529  0.835443                0.000000       0.000000       add   \n0  0.823529  0.835443                1.000000       0.799972    remove   \n0  0.822021  0.834270                0.997729       0.952570      swap   \n0  0.814480  0.827004                0.985744       0.472017      swap   \n0  0.814480  0.827004                0.984999       0.681380       add   \n0  0.811463  0.823695                0.979010       0.149849       add   \n0  0.811463  0.823695                0.977917       0.444702    remove   \n0  0.822021  0.833333                0.997066       0.416433      swap   \n0  0.817496  0.829817                0.987705       0.994286    remove   \n0  0.812971  0.825352                0.977468       0.550854       add   \n0  0.814480  0.827004                0.979648       0.963763      swap   \n0  0.815988  0.829132                0.982125       0.616452      swap   \n0  0.815988  0.829609                0.981193       0.472356      swap   \n0  0.814480  0.828452                0.976303       0.392998    remove   \n0  0.815988  0.830084                0.979183       0.941635      swap   \n0  0.815988  0.830084                0.978099       0.856068       add   \n0  0.811463  0.827586                0.963391       0.182061      swap   \n0  0.808446  0.824828                0.952112       0.355008    remove   \n0  0.809955  0.825967                0.954574       0.161674      swap   \n0  0.809955  0.825967                0.952241       0.488600    remove   \n0  0.808446  0.824828                0.944371       0.565986       add   \n0  0.808446  0.824828                0.941530       0.510700       add   \n0  0.809955  0.825967                0.944520       0.342247      swap   \n0  0.805430  0.822558                0.923015       0.233395    remove   \n0  0.803922  0.820937                0.912696       0.785633    remove   \n0  0.805430  0.822558                0.915062       0.155562      swap   \n0  0.805430  0.822558                0.910797       0.415385       add   \n0  0.805430  0.822558                0.906329       0.481141       add   \n0  0.808446  0.824828                0.917342       0.049432      swap   \n0  0.812971  0.829670                0.938408       0.684015       add   \n0  0.812971  0.829670                0.935273       0.664931    remove   \n0  0.812971  0.829670                0.931985       0.530398       add   \n0  0.812971  0.829670                0.928536       0.458805       add   \n0  0.815988  0.831956                0.945777       0.682665    remove   \n0  0.814480  0.829876                0.932003       0.335346      swap   \n0  0.817496  0.832178                0.951784       0.600212      swap   \n0  0.817496  0.832178                0.949312       0.761278       add   \n0  0.817496  0.832178                0.946716       0.635159    remove   \n0  0.815988  0.831025                0.930487       0.583567       add   \n0  0.811463  0.826147                0.885730       0.256136       add   \n0  0.802413  0.818308                0.799694       0.205468      swap   \n0  0.805430  0.820584                0.817358       0.462308       add   \n0  0.808446  0.823856                0.837855       0.331846       add   \n0  0.802413  0.816783                0.770504       0.844610       add   \n\n  Improved  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        0  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        1  \n0        0  \n0        1  \n0        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Iteration</th>\n      <th>Feature Count</th>\n      <th>Feature Set</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Acceptance Probability</th>\n      <th>Random Number</th>\n      <th>Operation</th>\n      <th>Improved</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>250</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.832178</td>\n      <td>0.995485</td>\n      <td>0.988850</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>249</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.819005</td>\n      <td>0.833333</td>\n      <td>0.996830</td>\n      <td>0.530471</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>250</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.832178</td>\n      <td>0.994999</td>\n      <td>0.379146</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.820513</td>\n      <td>0.832630</td>\n      <td>0.998242</td>\n      <td>0.425876</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.820513</td>\n      <td>0.832630</td>\n      <td>0.998150</td>\n      <td>0.999881</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.820513</td>\n      <td>0.832630</td>\n      <td>0.998053</td>\n      <td>0.856608</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.823529</td>\n      <td>0.835443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.823529</td>\n      <td>0.835443</td>\n      <td>1.000000</td>\n      <td>0.799972</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.822021</td>\n      <td>0.834270</td>\n      <td>0.997729</td>\n      <td>0.952570</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.814480</td>\n      <td>0.827004</td>\n      <td>0.985744</td>\n      <td>0.472017</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.814480</td>\n      <td>0.827004</td>\n      <td>0.984999</td>\n      <td>0.681380</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.811463</td>\n      <td>0.823695</td>\n      <td>0.979010</td>\n      <td>0.149849</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>12</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.811463</td>\n      <td>0.823695</td>\n      <td>0.977917</td>\n      <td>0.444702</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.822021</td>\n      <td>0.833333</td>\n      <td>0.997066</td>\n      <td>0.416433</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>14</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.829817</td>\n      <td>0.987705</td>\n      <td>0.994286</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.812971</td>\n      <td>0.825352</td>\n      <td>0.977468</td>\n      <td>0.550854</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>16</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.814480</td>\n      <td>0.827004</td>\n      <td>0.979648</td>\n      <td>0.963763</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.829132</td>\n      <td>0.982125</td>\n      <td>0.616452</td>\n      <td>swap</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.829609</td>\n      <td>0.981193</td>\n      <td>0.472356</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.814480</td>\n      <td>0.828452</td>\n      <td>0.976303</td>\n      <td>0.392998</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.830084</td>\n      <td>0.979183</td>\n      <td>0.941635</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>21</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.830084</td>\n      <td>0.978099</td>\n      <td>0.856068</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.811463</td>\n      <td>0.827586</td>\n      <td>0.963391</td>\n      <td>0.182061</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.808446</td>\n      <td>0.824828</td>\n      <td>0.952112</td>\n      <td>0.355008</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.809955</td>\n      <td>0.825967</td>\n      <td>0.954574</td>\n      <td>0.161674</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>25</td>\n      <td>250</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.809955</td>\n      <td>0.825967</td>\n      <td>0.952241</td>\n      <td>0.488600</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>26</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.808446</td>\n      <td>0.824828</td>\n      <td>0.944371</td>\n      <td>0.565986</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>27</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.808446</td>\n      <td>0.824828</td>\n      <td>0.941530</td>\n      <td>0.510700</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>28</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.809955</td>\n      <td>0.825967</td>\n      <td>0.944520</td>\n      <td>0.342247</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>29</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.805430</td>\n      <td>0.822558</td>\n      <td>0.923015</td>\n      <td>0.233395</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n      <td>250</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.803922</td>\n      <td>0.820937</td>\n      <td>0.912696</td>\n      <td>0.785633</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>31</td>\n      <td>250</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.805430</td>\n      <td>0.822558</td>\n      <td>0.915062</td>\n      <td>0.155562</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>251</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.805430</td>\n      <td>0.822558</td>\n      <td>0.910797</td>\n      <td>0.415385</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>33</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.805430</td>\n      <td>0.822558</td>\n      <td>0.906329</td>\n      <td>0.481141</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>34</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.808446</td>\n      <td>0.824828</td>\n      <td>0.917342</td>\n      <td>0.049432</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>35</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.812971</td>\n      <td>0.829670</td>\n      <td>0.938408</td>\n      <td>0.684015</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>36</td>\n      <td>252</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.812971</td>\n      <td>0.829670</td>\n      <td>0.935273</td>\n      <td>0.664931</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>37</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.812971</td>\n      <td>0.829670</td>\n      <td>0.931985</td>\n      <td>0.530398</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>38</td>\n      <td>254</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.812971</td>\n      <td>0.829670</td>\n      <td>0.928536</td>\n      <td>0.458805</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.831956</td>\n      <td>0.945777</td>\n      <td>0.682665</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.814480</td>\n      <td>0.829876</td>\n      <td>0.932003</td>\n      <td>0.335346</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.832178</td>\n      <td>0.951784</td>\n      <td>0.600212</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>42</td>\n      <td>254</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.832178</td>\n      <td>0.949312</td>\n      <td>0.761278</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>43</td>\n      <td>253</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.817496</td>\n      <td>0.832178</td>\n      <td>0.946716</td>\n      <td>0.635159</td>\n      <td>remove</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>44</td>\n      <td>254</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.815988</td>\n      <td>0.831025</td>\n      <td>0.930487</td>\n      <td>0.583567</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>45</td>\n      <td>255</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.811463</td>\n      <td>0.826147</td>\n      <td>0.885730</td>\n      <td>0.256136</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>46</td>\n      <td>255</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.802413</td>\n      <td>0.818308</td>\n      <td>0.799694</td>\n      <td>0.205468</td>\n      <td>swap</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>47</td>\n      <td>256</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.805430</td>\n      <td>0.820584</td>\n      <td>0.817358</td>\n      <td>0.462308</td>\n      <td>add</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>48</td>\n      <td>257</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.808446</td>\n      <td>0.823856</td>\n      <td>0.837855</td>\n      <td>0.331846</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>49</td>\n      <td>258</td>\n      <td>[967, 51, 649, 840, 270, 776, 415, 155, 331, 8...</td>\n      <td>0.802413</td>\n      <td>0.816783</td>\n      <td>0.770504</td>\n      <td>0.844610</td>\n      <td>add</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_benchmark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second implementation  -  WRONG\n",
    "based on vector of ones and zeros to record which feature to choose"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def simulated_annealing(df, labels, initial_temp, final_temp, alpha, num_features_target):\n",
    "    num_features = df.shape[1]\n",
    "    current_vector = np.random.choice([0, 1], size=num_features)\n",
    "    # Ensure the initial vector has exactly num_features_target features\n",
    "    current_vector = adjust_vector_to_target(current_vector, num_features_target)\n",
    "    best_vector = current_vector.copy()\n",
    "\n",
    "    best_accuracy, best_f1 = evaluate_attribute_set(best_vector, df, labels)\n",
    "\n",
    "    current_temp = initial_temp\n",
    "    while current_temp > final_temp:\n",
    "        new_vector = make_random_change(current_vector.copy(), num_features_target)\n",
    "        accuracy, f1 = evaluate_attribute_set(new_vector, df, labels)\n",
    "\n",
    "        if accept_change(best_accuracy, accuracy, current_temp):\n",
    "            current_vector = new_vector\n",
    "            if accuracy > best_accuracy:\n",
    "                best_vector = current_vector\n",
    "                best_accuracy = accuracy\n",
    "                best_f1 = f1\n",
    "\n",
    "        current_temp *= alpha\n",
    "\n",
    "    return best_vector, best_accuracy, best_f1\n",
    "\n",
    "def make_random_change(current_vector, num_features_target):\n",
    "    # Get indices where features are currently selected (1) and not selected (0)\n",
    "    indices_with_1 = np.where(current_vector == 1)[0]\n",
    "    indices_with_0 = np.where(current_vector == 0)[0]\n",
    "\n",
    "    # Check if the number of selected features matches num_features_target\n",
    "    if len(indices_with_1) > num_features_target:\n",
    "        # More features selected than desired, randomly remove features\n",
    "        indices_to_remove = np.random.choice(indices_with_1, size=(len(indices_with_1) - num_features_target), replace=False)\n",
    "        current_vector[indices_to_remove] = 0\n",
    "    elif len(indices_with_1) < num_features_target:\n",
    "        # Fewer features selected than desired, randomly add features\n",
    "        indices_to_add = np.random.choice(indices_with_0, size=(num_features_target - len(indices_with_1)), replace=False)\n",
    "        current_vector[indices_to_add] = 1\n",
    "    else:\n",
    "        # Exactly num_features_target features are selected, perform a swap\n",
    "        idx_to_remove = np.random.choice(indices_with_1)\n",
    "        idx_to_add = np.random.choice(indices_with_0)\n",
    "        current_vector[idx_to_remove] = 0\n",
    "        current_vector[idx_to_add] = 1\n",
    "\n",
    "    return current_vector\n",
    "\n",
    "def adjust_vector_to_target(vector, num_features_target):\n",
    "    # Adjust the vector to have exactly num_features_target features\n",
    "    current_count = np.sum(vector)\n",
    "    while current_count != num_features_target:\n",
    "        if current_count < num_features_target:\n",
    "            idx_to_add = np.random.choice(np.where(vector == 0)[0])\n",
    "            vector[idx_to_add] = 1\n",
    "        elif current_count > num_features_target:\n",
    "            idx_to_remove = np.random.choice(np.where(vector == 1)[0])\n",
    "            vector[idx_to_remove] = 0\n",
    "        current_count = np.sum(vector)\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "N_OF_FEATURES_TO_TEST = 100\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :N_OF_FEATURES_TO_TEST], labels, test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.iloc[:, :N_OF_FEATURES_TO_TEST], labels, test_size=0.2,\n",
    "                                                    random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[83], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m      2\u001B[0m num_features_target \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m  \u001B[38;5;66;03m# Set the number of features you want to select\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m selected_vector, accuracy, f1 \u001B[38;5;241m=\u001B[39m \u001B[43msimulated_annealing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_temp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_temp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_features_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_features_target\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 13\u001B[0m, in \u001B[0;36msimulated_annealing\u001B[1;34m(df, labels, initial_temp, final_temp, alpha, num_features_target)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m current_temp \u001B[38;5;241m>\u001B[39m final_temp:\n\u001B[0;32m     12\u001B[0m     new_vector \u001B[38;5;241m=\u001B[39m make_random_change(current_vector\u001B[38;5;241m.\u001B[39mcopy(), num_features_target)\n\u001B[1;32m---> 13\u001B[0m     accuracy, f1 \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_attribute_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_vector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accept_change(best_accuracy, accuracy, current_temp):\n\u001B[0;32m     16\u001B[0m         current_vector \u001B[38;5;241m=\u001B[39m new_vector\n",
      "Cell \u001B[1;32mIn[40], line 13\u001B[0m, in \u001B[0;36mevaluate_attribute_set\u001B[1;34m(attribute_names, df, labels)\u001B[0m\n\u001B[0;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m train_knn(X_train, y_train)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m accuracy, f1 \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy, f1\n",
      "Cell \u001B[1;32mIn[39], line 18\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(model, X_test, y_test)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_model\u001B[39m(model, X_test, y_test):\n\u001B[0;32m      6\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m    Evaluate a logistic regression model on the test data.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m    - report: str, classification report (includes precision, recall, f1-score, and support)\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n\u001B[0;32m     20\u001B[0m     f1 \u001B[38;5;241m=\u001B[39m f1_score(y_test, y_pred)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001B[0m, in \u001B[0;36mKNeighborsClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m    Class labels for each data sample.\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muniform\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;66;03m# In that case, we do not need the distances to perform\u001B[39;00m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;66;03m# the weighting so we do not compute them.\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m     neigh_ind \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m     neigh_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\neighbors\\_base.py:806\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    804\u001B[0m         X \u001B[38;5;241m=\u001B[39m _check_precomputed(X)\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 806\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    808\u001B[0m n_samples_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_fit_\n\u001B[0;32m    809\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_neighbors \u001B[38;5;241m>\u001B[39m n_samples_fit:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:546\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    545\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 546\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    547\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:761\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    759\u001B[0m dtypes_orig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    760\u001B[0m pandas_requires_conversion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(array, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtypes\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[43marray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtypes\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__array__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;66;03m# throw warning if columns are sparse. If all columns are sparse, then\u001B[39;00m\n\u001B[0;32m    763\u001B[0m     \u001B[38;5;66;03m# array.sparse exists and sparsity will be preserved (later).\u001B[39;00m\n\u001B[0;32m    764\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m suppress(\u001B[38;5;167;01mImportError\u001B[39;00m):\n\u001B[0;32m    765\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_sparse\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:6073\u001B[0m, in \u001B[0;36mNDFrame.dtypes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   6045\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m   6046\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdtypes\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   6047\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   6048\u001B[0m \u001B[38;5;124;03m    Return the dtypes in the DataFrame.\u001B[39;00m\n\u001B[0;32m   6049\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   6071\u001B[0m \u001B[38;5;124;03m    dtype: object\u001B[39;00m\n\u001B[0;32m   6072\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 6073\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dtypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6074\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor_sliced(data, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mobject_)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\managers.py:272\u001B[0m, in \u001B[0;36mBaseBlockManager.get_dtypes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dtypes\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 272\u001B[0m     dtypes \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mblk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mblk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblknos)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_features_target = 5  # Set the number of features you want to select\n",
    "selected_vector, accuracy, f1 = simulated_annealing(df, labels, initial_temp=1000, final_temp=1, alpha=0.95, num_features_target=num_features_target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf",
   "language": "python",
   "display_name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}